import pandas as pd
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score, precision_score
import matplotlib.pyplot as plt
import seaborn as sns
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)

# Step 1: Load Data
try:
    data_path = 'C:/Users/APOORVA DHIMAN/OneDrive/Desktop/Malware project/data_s/TUANDROMD.csv'
    data = pd.read_csv(data_path)
    logging.info("Data loaded successfully.")
except Exception as e:
    logging.error(f"Error loading data: {e}")
    raise

# Step 2: Data Preprocessing
try:
    logging.info("Inspecting dataset columns and data types...")
    logging.info(data.info())
    logging.info("Dataset columns: %s", data.columns)
    logging.info("Dataset preview: \n%s", data.head())

    target_column = 'Label'  # Update this to your actual target column name

    if target_column not in data.columns:
        raise ValueError(f"Target column '{target_column}' not found in dataset")

    X = data.drop(target_column, axis=1)
    y = data[target_column].apply(lambda x: 1 if x == 'malware' else 0)

    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors='coerce')
    X = X.select_dtypes(include=['number'])

    if X.isnull().sum().sum() > 0:
        X.fillna(X.mean(), inplace=True)
        logging.info("Handled missing values in features by filling with mean.")

    X.dropna(axis=0, inplace=True)
    y = y[X.index]

    y = pd.to_numeric(y, errors='coerce').dropna().astype(int)

    logging.info("Data preprocessing completed.")
except Exception as e:
    logging.error(f"Error in data preprocessing: {e}")
    raise

# Step 3: Split Data
try:
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    logging.info("Data split into training and testing sets.")
except Exception as e:
    logging.error(f"Error in data splitting: {e}")
    raise

# Step 4: Model Building with Hyperparameter Tuning
try:
    model = RandomForestClassifier(random_state=42)
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }

    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)
    grid_search.fit(X_train, y_train)

    best_model = grid_search.best_estimator_
    logging.info("Model training and hyperparameter tuning completed.")
except Exception as e:
    logging.error(f"Error in model training: {e}")
    raise

# Step 5: Cross-Validation
try:
    # Perform 5-fold cross-validation
    cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='accuracy')
    logging.info("Cross-validation scores: %s", cv_scores)
    logging.info("Mean cross-validation score: %f", cv_scores.mean())
except Exception as e:
    logging.error(f"Error in cross-validation: {e}")
    raise

# Step 6: Make Predictions
try:
    y_pred = best_model.predict(X_test)
    y_pred_prob = best_model.predict_proba(X_test)[:, 1]
    logging.info("Model predictions made.")
except Exception as e:
    logging.error(f"Error in making predictions: {e}")
    raise

# Step 7: Evaluate Model
try:
    conf_matrix = confusion_matrix(y_test, y_pred)
    logging.info("Confusion Matrix: \n%s", conf_matrix)
    logging.info("\nClassification Report:\n%s", classification_report(y_test, y_pred))

    # ROC-AUC Score
    roc_auc = roc_auc_score(y_test, y_pred_prob)
    logging.info("\nROC-AUC Score: %f", roc_auc)

    # Accuracy
    accuracy = accuracy_score(y_test, y_pred)
    logging.info("\nAccuracy: %f", accuracy)

    # Precision
    precision = precision_score(y_test, y_pred)
    logging.info("\nPrecision: %f", precision)

    # ROC Curve
    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc="lower right")
    plt.show()
    logging.info("Model evaluation completed.")
except Exception as e:
    logging.error(f"Error in model evaluation: {e}")
    raise

# Step 8: Visualization
try:
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()
    logging.info("Confusion matrix visualized.")
except Exception as e:
    logging.error(f"Error in visualization: {e}")
    raise

# Step 9: Save Model
try:
    model_filename = 'tuandromd_detector.pkl'
    model_path = f'C:/Users/APOORVA DHIMAN/OneDrive/Desktop/Malware project/{model_filename}'
    joblib.dump(best_model, model_path)
    logging.info(f"Model saved to {model_path}")
except Exception as e:
    logging.error(f"Error in saving the model: {e}")
    raise

# Step 10: Plot Bar Graph for Metrics
try:
    metrics = {
        'Accuracy': accuracy,
        'ROC-AUC Score': roc_auc,
        'Precision': precision
    }

    metrics_names = list(metrics.keys())
    metrics_values = list(metrics.values())

    plt.figure(figsize=(10, 6))
    plt.bar(metrics_names, metrics_values, color=['blue', 'orange', 'green'])
    plt.ylim(0, 1)
    plt.xlabel('Evaluation Metrics')
    plt.ylabel('Score')
    plt.title('Model Evaluation Metrics')
    for index, value in enumerate(metrics_values):
        plt.text(index, value + 0.01, f"{value:.2f}", ha='center')
    plt.show()
    logging.info("Evaluation metrics bar graph plotted.")
except Exception as e:
    logging.error(f"Error in plotting evaluation metrics: {e}")
    raise
